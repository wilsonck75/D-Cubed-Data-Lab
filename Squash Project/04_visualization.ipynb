{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aae0b84",
   "metadata": {},
   "source": [
    "# Visualizations\n",
    "\n",
    "Visualize session patterns, types, and revenue distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f296a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“¦ Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afadf348",
   "metadata": {},
   "source": [
    "## Graphs and Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c781abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Importing Essential Libraries for Data Analysis, Visualization, Statistical Modeling, and Machine Learning\n",
    "#############################\n",
    "\n",
    "# -----------------------------\n",
    "# Basic Libraries and Environment Setup\n",
    "# -----------------------------\n",
    "import warnings                          # For handling warnings\n",
    "warnings.filterwarnings(\"ignore\")        # Suppress warnings for cleaner output\n",
    "\n",
    "import numpy as np                       # Numerical operations and basic array handling\n",
    "import pandas as pd                      # Data manipulation and analysis\n",
    "import matplotlib.pyplot as plt          # Basic plotting functions\n",
    "import seaborn as sns                    # High-level statistical data visualization\n",
    "from IPython.display import display, HTML  # Display enhancements in IPython notebooks\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# -----------------------------\n",
    "# Data Manipulation and Statistical Analysis\n",
    "# -----------------------------\n",
    "# SciPy and Statsmodels for advanced statistical analysis and estimation.\n",
    "from scipy.stats import gaussian_kde, pearsonr, spearmanr, chi2_contingency  # Statistical functions and tests\n",
    "import scipy.stats as st                 # Additional statistical functions\n",
    "from scipy.spatial.distance import cdist, pdist  # Distance calculations for clustering\n",
    "\n",
    "import statsmodels.api as sm             # Comprehensive statistical analysis\n",
    "import statsmodels.formula.api as smf      # Formula-based statistical models\n",
    "from statsmodels.graphics.mosaicplot import mosaic  # Mosaic plots for categorical data visualization\n",
    "import statsmodels.stats.api as sms        # For hypothesis testing and statistical details\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor  # Multicollinearity diagnostics\n",
    "from statsmodels.tools.tools import add_constant  # Add intercept term in regression models\n",
    "\n",
    "# -----------------------------\n",
    "# Machine Learning and Clustering Libraries (scikit-learn and imbalanced-learn)\n",
    "# -----------------------------\n",
    "# Datasets, Preprocessing, and Model Selection\n",
    "from sklearn import datasets             # Access built-in datasets like Iris\n",
    "from sklearn.decomposition import PCA    # Principal Component Analysis for dimensionality reduction\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer  # Preprocessing tools: scaling, encoding, custom transformations\n",
    "from sklearn.impute import SimpleImputer   # Imputation for missing values\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,                    # Split the dataset into training and testing subsets\n",
    "    GridSearchCV,                        # Exhaustive search over specified parameter values\n",
    "    RandomizedSearchCV,                  # Randomized search for hyperparameters\n",
    "    learning_curve                       # Generate learning curves to diagnose model performance\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer  # Combine multiple preprocessing steps\n",
    "from sklearn.pipeline import Pipeline      # Pipeline for sequential data processing and modeling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Clustering and Classification Algorithms\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering  # Common clustering algorithms\n",
    "from sklearn.neighbors import KNeighborsClassifier  # K-Nearest Neighbors classifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor  # Decision trees for classification and regression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor  # Ensemble methods: Random Forests\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression  # Linear and logistic regression models\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import (\n",
    "    f1_score,                           # F1 score, harmonic mean of precision and recall\n",
    "    accuracy_score,                     # Accuracy metric\n",
    "    recall_score,                       # Recall metric\n",
    "    precision_score,                    # Precision metric\n",
    "    confusion_matrix,                   # Confusion matrix to summarize model predictions\n",
    "    roc_auc_score,                      # Area Under the ROC Curve\n",
    "    classification_report,              # Detailed classification report\n",
    "    precision_recall_curve,             # Precision-Recall curve data\n",
    "    roc_curve,                          # ROC curve data\n",
    "    make_scorer,                        # Create custom scoring functions\n",
    "    silhouette_score                    # Evaluate clustering quality\n",
    ")\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Imbalanced Data Handling\n",
    "from imblearn.over_sampling import SMOTE  # Synthetic Minority Over-sampling Technique for balancing classes\n",
    "\n",
    "# -----------------------------\n",
    "# Miscellaneous Utilities and Settings\n",
    "# -----------------------------\n",
    "import math  # For mathematical functions like ceiling and floor\n",
    "from pprint import pprint\n",
    "\n",
    "# Set the visual theme for seaborn plots for a consistent aesthetic.\n",
    "sns.set_theme(style='darkgrid')\n",
    "\n",
    "# Adjust pandas display options to ensure all columns and up to 200 rows are displayed.\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a6505",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_revenue_df['Revenue'].hist(bins=400, figsize=(10, 6))\n",
    "plt.suptitle('Distribution of Revenue by User', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use features: Revenue and Number of Transactions\n",
    "features = user_data[['Revenue', 'Num_Transactions']]\n",
    "\n",
    "# Scale the features for clustering\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Run K-Means clustering with 3 clusters (same as current segments)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "user_data['KMeans_Cluster'] = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=user_data, x='Num_Transactions', y='Revenue', hue='KMeans_Cluster', palette='Set2')\n",
    "plt.title('K-Means Clustering of Users (Based on Revenue and Transactions)')\n",
    "plt.xlabel('Number of Transactions')\n",
    "plt.ylabel('Revenue ($)')\n",
    "plt.legend(title='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show sample counts per cluster\n",
    "user_data['KMeans_Cluster'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a3472",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_freq = df.groupby('Full Name').size().reset_index(name='Num_Sessions')\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(user_freq['Num_Sessions'], bins=30, kde=True)\n",
    "plt.title('Distribution of Sessions per User')\n",
    "plt.xlabel('Number of Sessions')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f25e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values(by=['Full Name', 'Session Date'])\n",
    "df_sorted['Prev Session Date'] = df_sorted.groupby('Full Name')['Session Date'].shift(1)\n",
    "df_sorted['Days Between Sessions'] = (df_sorted['Session Date'] - df_sorted['Prev Session Date']).dt.days\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(df_sorted['Days Between Sessions'].dropna(), bins=30, kde=True)\n",
    "plt.title('Days Between Sessions')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Session Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(DF_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a3cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_users = DF_play['Full Name'].value_counts().head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_users.values, y=top_users.index, orient='h')\n",
    "plt.title('Top 10 Users by Entry Count')\n",
    "plt.xlabel('Number of Entries')\n",
    "plt.ylabel('User (Full Name)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac61d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_distribution = DF_play['Full Name'].value_counts().value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=entry_distribution.index, y=entry_distribution.values)\n",
    "plt.title('Distribution of Entry Counts Across Users')\n",
    "plt.xlabel('Number of Entries per User')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
